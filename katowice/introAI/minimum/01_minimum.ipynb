{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c1b9ab4d37b3676",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Numeryczne szukanie minimum funkcji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e549abb65653966d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Numerical search for the minimum function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ee70ea63f3a125e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-220464f9c62a6f30",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Jest to najprostsza metoda szukania minimum funkcji $f(x)$. Argumentem funkcji w ogólności może być wektor $\\overline{x}$ lub w najprostszym przypadku (rozpatrywanym teraz) liczba. Jest to metoda iteracyjna, zaczyna od pewnego punktu w przestrzeni parametrów i w każdym kroku przesuwa ten punkt w stronę wskazywaną przez gradient funkcji.\n",
    "\n",
    "$$x_{i+1} =x_{i}-\\alpha \\frac{\\partial f}{\\partial x}(x_i)  $$\n",
    "\n",
    "Znak minus bierze się stąd, że gradient wskazuje kierunek najszybszego rośnięcia funkcji, a chcemy szukać minimum. Parametr $\\alpha$ to długość kroku, należy go dobrać odpowiednio do zagadnienia. Zbyt mała wartość może spowodować, że metoda będzie działać powoli lub utknie w minimum lokalnym, zbyt duża wartość spowoduje, że program nigdy nie znajdzie dokładnej odpowiedzi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cf0a50f51f7d4d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "This is the simplest method of finding the minimum $f(x)$ function. The argument of the function in general can be the $\\overline{x}$ vector or in the simplest case (now considered) a number. It is an iterative method, it starts at a certain point in the parameter space and at each step shifts this point in the direction indicated by the function gradient.\n",
    "\n",
    "$$x_{i+1} =x_{i}-\\alpha \\frac{\\partial f}{\\partial x}(x_i)  $$\n",
    "\n",
    "The minus sign comes from the fact that the gradient indicates the direction of the fastest growth of the function, and we want to look for the minimum. The $\\alpha$ parameter is the length of the step, it should be chosen according to the problem to be solved. Too small value may cause the method to work slowly or get stuck in the local minimum, too large value will cause that the program will never find the exact answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-19b3fb5a8397f8a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**1\\. Funkcja kwadratowa**\n",
    "\n",
    "Jedną z najprostszych funkcji, na których można ćwiczyć to funkcja kwadratowa $f(x)=ax^2+bx+c$.\n",
    "\n",
    "Jej gradient wynosi $\\frac{\\partial f}{\\partial x}=2ax+b$\n",
    "\n",
    "Zbierając to razem dostajemy następujący wzór (nie pomylić alfy z a):\n",
    "\n",
    "$$x_{i+1} =x_{i}-\\alpha (2ax_i+b)  $$\n",
    "\n",
    "Jako punkt początkowy $x_0$ należy wybrać losowo liczbę z przedziału [-10,10], parametr \"steps\" to ilość kroków optymalizacji, które ma wykonać program. Jako długość kroku $\\alpha$ można wziąć na początek 0.01.\n",
    "\n",
    "Funkcja SGD1 powinna zwrócić wartość x dla którego $f(x)$ ma minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-146db21fcbe868ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**1 \\. Quadratic function**\n",
    "\n",
    "One of the simplest functions you can practice is the square function $f(x)=ax^2+bx+c$.\n",
    "\n",
    "Its gradient is $\\frac{\\partial f}{\\partial x}=2ax+b$\n",
    "\n",
    "Collecting this together we get the following formula (do not confuse the alpha with a):\n",
    "\n",
    "$$x_{i+1} =x_{i}-\\alpha (2ax_i+b)  $$\n",
    "\n",
    "As the starting point of $x_0$, randomly select a number from the range [-10.10], the \"steps\" parameter is the number of optimization steps that the program is to perform. You can take 0.01 at the beginning of the $\\alpha$ step length.\n",
    "\n",
    "SGD1 should return x for which $f(x)$ has a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-431e25a74edef45d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD1(a, b, c, steps):\n",
    "    ### BEGIN SOLUTION\n",
    "    alfa = 0.001\n",
    "    x = -10 + 20 * np.random.uniform()\n",
    "    for i in range(0, steps):\n",
    "        grad = 2 * a * x + b\n",
    "        x = x - alfa * grad\n",
    "    return x\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b23fb30d26f8ff4b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(SGD1(2, -3, 1, 10000), 0.75)\n",
    "np.testing.assert_almost_equal(SGD1(2, 2, 1.23, 10000), -0.5)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "np.testing.assert_almost_equal(SGD1(5.64, 2.36, 0, 10000), -2.36 / (2 * 5.64))\n",
    "np.testing.assert_almost_equal(SGD1(2.45, 34.31, 1, 10000), -34.31 / (2 * 2.45))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42cbed9eb30c75b8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**2\\. Regresja liniowa**\n",
    "\n",
    "Regresja liniowa to metoda znajdowania parametrów prostej $y=ax+b$ która najlepiej opisuje dane. Najczęściej robi się to [metodą najmniejszych kwadratów](https://pl.wikipedia.org/wiki/Metoda_najmniejszych_kwadrat%C3%B3w), która polega na zminimalizowaniu następującej funkcji:\n",
    "\n",
    "$$L(a,b)=\\sum_{j=0}^{n-1} (y_j-ax_j-b)^2$$\n",
    "\n",
    "gdzie $x_i,y_i$ to odpowiednia para danych do których dopasowujemy prostą.\n",
    "\n",
    "Gradient L ze względu na a i b jest równy:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a}=-2\\sum_{j=0}^{n-1}(y_j-ax_j-b)x_j$$\n",
    "$$\\frac{\\partial L}{\\partial b}=-2\\sum_{j=0}^{n-1}(y_j-ax_j-b)$$\n",
    "\n",
    "Tym razem równania wyglądają następująco\n",
    "$$a_{i+1}=a_i-\\alpha\\frac{\\partial L}{\\partial a}=a_i+2\\alpha\\sum_{j=0}^{n-1}(y_j-ax_j-b)x_j$$\n",
    "$$b_{i+1}=b_i-\\alpha\\frac{\\partial L}{\\partial b}=b_i+2\\alpha\\sum_{j=0}^{n-1}(y_j-ax_j-b)$$\n",
    "\n",
    "\n",
    "Funksja SGD_reglin powinna zwrócić np.array z wartościami znalezionych a i b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7743328af7e9e47e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2 \\. Linear regression**\n",
    "\n",
    "Linear regression is a method of finding the $y=ax+b$ straight line parameters that best describes the data. Most often it is done [least squares method] (https://en.wikipedia.org/wiki/Least_squares), which consists in minimizing the following function:\n",
    "\n",
    "$$L(a,b)=\\sum_{j=0}^{n-1} (y_j-ax_j-b)^2$$\n",
    "\n",
    "where $x_i,y_i$ is the right data pair for which we match the straight line.\n",
    "\n",
    "The gradient L due to a and b is equal to:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a}=-2\\sum_{j=0}^{n-1}(y_j-ax_j-b)x_j$$\n",
    "$$\\frac{\\partial L}{\\partial b}=-2\\sum_{j=0}^{n-1}(y_j-ax_j-b)$$\n",
    "\n",
    "This time the equations look like this\n",
    "$$a_{i+1}=a_i-\\alpha\\frac{\\partial L}{\\partial a}=a_i+2\\alpha\\sum_{j=0}^{n-1}(y_j-ax_j-b)x_j$$\n",
    "$$b_{i+1}=b_i-\\alpha\\frac{\\partial L}{\\partial b}=b_i+2\\alpha\\sum_{j=0}^{n-1}(y_j-ax_j-b)$$\n",
    "\n",
    "\n",
    "The SGD_reglin function should return np.array with values of found a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3cbf264c883c8209",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generowanie danych do regresji liniowej\n",
    "def genData(n, a, b, useNoise):\n",
    "    result = np.zeros((n, 2)) #x_i=result[i][0], y_i=result[i][1]\n",
    "    for i in range(0, n):\n",
    "        x = -10 + 20 * np.random.uniform()\n",
    "        result[i][0] = x\n",
    "        result[i][1] = a * x + b\n",
    "        if useNoise == True:\n",
    "            result[i][1] += 0.1 * np.random.normal()\n",
    "    return result\n",
    "\n",
    "data_reglin1 = genData(100, 2, -3, False)\n",
    "data_reglin2 = genData(100, 0.123, 1.4142, True)\n",
    "\n",
    "answer_reglin1 = np.polyfit(data_reglin1[:,0], data_reglin1[:,1],1)\n",
    "answer_reglin2 = np.polyfit(data_reglin2[:,0], data_reglin2[:,1],1)\n",
    "print(answer_reglin1)\n",
    "print(answer_reglin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-760e13648dbd2375",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD_reglin(data, alfa, steps):\n",
    "    a = 0.0\n",
    "    b = 0.0\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    ### BEGIN SOLUTION\n",
    "    for i in range(steps):\n",
    "        grad_a = np.dot(y - a * x - b, x) \n",
    "        grad_b = np.sum(y - a * x - b)\n",
    "        a += 2 * alfa * grad_a\n",
    "        b += 2 * alfa * grad_b\n",
    "    return np.array((a, b))\n",
    "    ### END SOLUTION\n",
    "    \n",
    "print(SGD_reglin(data_reglin1, 0.0001, 10000))\n",
    "print(SGD_reglin(data_reglin2, 0.0001, 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d2c7ea1e71132f51",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "answer_reglin1=np.polyfit(data_reglin1[:,0],data_reglin1[:,1],1)\n",
    "answer_reglin2=np.polyfit(data_reglin2[:,0],data_reglin2[:,1],1)\n",
    "np.testing.assert_almost_equal(SGD_reglin(data_reglin1,0.0001,10000),np.array(answer_reglin1))\n",
    "np.testing.assert_almost_equal(SGD_reglin(data_reglin2,0.0001,10000),np.array(answer_reglin2))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "data_reglin3=genData(100,-0.12,0.24,True)\n",
    "answer_reglin3=np.polyfit(data_reglin3[:,0],data_reglin3[:,1],1)\n",
    "np.testing.assert_almost_equal(SGD_reglin(data_reglin3,0.0001,10000),np.array(answer_reglin3))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bec11e8c99c8b2cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**3\\. Funkcja Rosenbrocka**\n",
    "\n",
    "Zadanie polega na znalezieniu minimum następującej funkcji $f(x,y)=(a-x)^2+b(y-x^2)^2$.\n",
    "\n",
    "\n",
    "Funkcja SGD_rosenbrok powinna zwrócić np.array z wartościami znalezionych x i y. Zacznij od losowo wybranych $x_0, y_0 \\in [-a,a]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e09a77011c0a781",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3 \\. Rosenbrock function**\n",
    "\n",
    "The task is to find the minimum of the following $f(x,y)=(a-x)^2+b(y-x^2)^2$ function.\n",
    "\n",
    "\n",
    "The SGD_rosenbrok function should return np.array with values of found x and y. Start with randomly selected $x_0, y_0 \\in [-a,a]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d4a8f8ccb281064",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD_rosenbrock(a, b, alfa, steps):\n",
    "    ### BEGIN SOLUTION\n",
    "    x = -a + 2 * a * np.random.uniform()\n",
    "    y = -a + 2 * a * np.random.uniform()\n",
    "    for i in range(steps):\n",
    "        grad_x = -2 * (a - x) + 2 * b * (y - x**2) * (-2 * x)\n",
    "        grad_y = 2 * b * (y - x**2)\n",
    "        x -= alfa * grad_x\n",
    "        y -= alfa * grad_y\n",
    "    return np.array((x, y))\n",
    "    ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-859cdf5fda9c3009",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(SGD_rosenbrock(-1, 100, 0.001, 1000000),np.array((-1, 1)))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "np.testing.assert_almost_equal(SGD_rosenbrock(0.7, 20, 0.001, 1000000),np.array((0.7, 0.49)))\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-019be2511a0956f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**4\\. Fitowanie wielomianu**\n",
    "\n",
    "Tym razem należy dopasować wielomian p(x) stopnia $N$.\n",
    "\n",
    "$$L(\\overline{a})=\\sum_{i=0}^{n-1} (y_i-p(x_i))^2$$\n",
    "$$p(x)=\\sum_{j=0}^{N} a_j x^j$$\n",
    "\n",
    "Gradient L ze względu na $a_j$ jest równy:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_j}=-2\\sum_{i=0}^{n-1}(y_i-p(x_i))x_i^j$$\n",
    "\n",
    "\n",
    "Funkcja SGD_polyfit powinna zwrócić np.array z wartościami znalezionych współczynników $a_0, a_1, a_2...$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-56700a6a2c6b2167",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**4 \\. Polynomial fitting**\n",
    "\n",
    "This time it is necessary to match the p(x) polynomial of degree $N$.\n",
    "\n",
    "$$L(\\overline{a})=\\sum_{i=0}^{n-1} (y_i-p(x_i))^2$$\n",
    "$$p(x)=\\sum_{j=0}^{N} a_j x^j$$\n",
    "\n",
    "The gradient of L due to $a_j$ is equal to:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_j}=-2\\sum_{i=0}^{n-1}(y_i-p(x_i))x_i^j$$\n",
    "\n",
    "\n",
    "The SGD_polyfit function should return np.array with values of the found $a_0, a_1, a_2...$ coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-343658226c6ad335",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generowanie danych do następnych zadań\n",
    "tmp_x = np.arange(-1, 1, 0.25)\n",
    "tmp_y = np.polynomial.polynomial.polyval(tmp_x,np.array((-0.1, 0.4, -0.5)))\n",
    "data_polyfit = np.zeros((len(tmp_x), 2))\n",
    "for i in range(len(tmp_x)):\n",
    "    data_polyfit[i][0] = tmp_x[i]\n",
    "    data_polyfit[i][1] = tmp_y[i] + np.random.uniform(-0.1, 0.1)\n",
    "answer_polyfit=np.polyfit(data_polyfit[:,0],data_polyfit[:,1],2)\n",
    "answer_polyfit=answer_polyfit[::-1]\n",
    "\n",
    "print(answer_polyfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07472a9864c73007",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD_polyfit(data, N, alfa, steps):\n",
    "    ### BEGIN SOLUTION\n",
    "    a = np.zeros(N + 1)\n",
    "    grad_a = np.zeros(N + 1)\n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    for i in range(steps):\n",
    "        tmp = y - np.polynomial.polynomial.polyval(x, a)\n",
    "        for j in range(N+1):\n",
    "            grad_a[j] = -2 * np.dot(tmp, x**j) \n",
    "        a -= alfa * grad_a\n",
    "    return a\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01dc65c48ab25060",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "out = SGD_polyfit(data_polyfit, 2, 0.001, 250000)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e8df4057fae5929",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(int(round(100*out[0])), int(round(100*answer_polyfit[0])))\n",
    "np.testing.assert_almost_equal(int(round(100*out[1])), int(round(100*answer_polyfit[1])))\n",
    "np.testing.assert_almost_equal(int(round(100*out[2])), int(round(100*answer_polyfit[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68031eb8caa53fa8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### W poniższych komórkach można zobaczyć efekt fitowania wielomianu stopnia 10 do 8 punktów danych oraz porównać wynik z prawidłowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78285c328eaea9e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In the cells below, you can see the effect of fitting a polynomial of 10th degree to 8 data points and compare the result with the correct one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = SGD_polyfit(data_polyfit, 10, 0.0005, 200000)\n",
    "print(pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8012eb74cdce02a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "iksy = np.arange(-4, 4, 0.02)\n",
    "igreki = np.polynomial.polynomial.polyval(iksy, pol)\n",
    "\n",
    "plt.axis([-1.5, 1.5, -1.5, 0.5])\n",
    "plt.plot(data_polyfit[:,0],data_polyfit[:,1],'o', color='red', label='data')\n",
    "plt.plot(iksy, np.polynomial.polynomial.polyval(iksy, answer_polyfit), '--', color='gray', label='correct')\n",
    "plt.plot(iksy, igreki, '-', color='blue', label='fit')\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33035e3279e9e9a2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Regularyzacja L2\n",
    "\n",
    "Jak widać w poprzednim zadaniu, próba dopasowania wielomianu o zbyt dużym stopniu do zbyt małej ilości danych prowadzi do przefitowania. Przefitowanie jest częstym problemem w zastosowaniach uczenia maszynowego. Najprostszym rozwiązaniem jest zwiększenie ilości danych, co jednak nie zawsze jest możliwe (a czasami nawet to nie pomaga). Inną metodą jest regularyzacja L2. Polega ona na dodaniu dodatkowego członu do funkcji błędu, który wymusza stosowanie małych wartości parametrów.\n",
    "\n",
    "$$L(\\overline{a})=\\sum_{i=0}^{n-1} (y_i-p(x_i))^2 + \\eta \\sum_{j=0}^{N} a_{j}^2 $$\n",
    "\n",
    "gdzie $N$ to stopień wielomianu.\n",
    "\n",
    "Zmieniony gradient L ze względu na $a_j$ jest równy:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_j}=-2\\sum_{i=0}^{n-1}(y_i-p(x_i))x_i^j+ \\eta a_j$$\n",
    "\n",
    "Na początek można wziąć $\\eta=10^{-4}$ i stopniowo zwiększać. Zauważ, że dla $\\eta=0$ program powinien zwracać dokładnie te same wyniki co w poprzednim zadaniu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-302a051c79b05d88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## L2 regularization\n",
    "\n",
    "As you can see in the previous task, attempting to fit a polynomial with too high degree to too little data leads to overfitting. Overfitting is a common problem in machine learning applications. The simplest solution is to increase the amount of data which, however, is not always possible (and sometimes even it does not help). L2 regularization is another method. It consists in adding an additional element to the error function, which forces the use of small parameter values.\n",
    "\n",
    "$$L(\\overline{a})=\\sum_{i=0}^{n-1} (y_i-p(x_i))^2 + \\eta \\sum_{j=0}^{N} a_{j}^2 $$\n",
    "\n",
    "where $N$ is the degree of polynomial.\n",
    "\n",
    "The changed gradient of L due to $a_j$ is equal to:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_j}=-2\\sum_{i=0}^{n-1}(y_i-p(x_i))x_i^j+ \\eta a_j$$\n",
    "\n",
    "To start with, you can take $\\eta=10^{-4}$ and increase it gradually. Note that for $\\eta=0$ the program should return exactly the same results as in the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a5de5893485f1f51",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def SGD_polyfit_L2(data, N, alfa, steps, eta):\n",
    "    ### BEGIN SOLUTION\n",
    "    a = np.zeros(N + 1)\n",
    "    grad_a = np.zeros(N + 1)\n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    for i in range(steps):\n",
    "        tmp = y - np.polynomial.polynomial.polyval(x, a)\n",
    "        for j in range(N+1):\n",
    "            grad_a[j] = -2 * np.dot(tmp, x**j) + eta * a[j]\n",
    "        a -= alfa * grad_a\n",
    "    return a\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_L2 = SGD_polyfit_L2(data_polyfit, 10, 0.0005, 200000, 0.25)\n",
    "print(pol_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6801845ed6ba840b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "iksy = np.arange(-2, 2, 0.02)\n",
    "igreki = np.polynomial.polynomial.polyval(iksy, pol)\n",
    "igreki_L2 = np.polynomial.polynomial.polyval(iksy, pol_L2)\n",
    "plt.axis([-1.5, 1.5, -1.5, 0.5])\n",
    "\n",
    "plt.plot(data_polyfit[:,0],data_polyfit[:,1],'o', color='red', label='data')\n",
    "plt.plot(iksy,np.polynomial.polynomial.polyval(iksy,answer_polyfit),'--',color='gray', label='correct')\n",
    "plt.plot(iksy,igreki,'-',color='blue', label='fit')\n",
    "plt.plot(iksy,igreki_L2,'-',color='green', label='fit L2')\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-55ae46c2e5a6a0bb",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "out_noL2 = SGD_polyfit_L2(data_polyfit, 8, 0.0005, 200000, 0.0)\n",
    "out_L2 = SGD_polyfit_L2(data_polyfit, 8, 0.0005, 200000, 0.5)\n",
    "np.testing.assert_array_less(np.dot(out_L2, out_L2), np.dot(out_noL2, out_noL2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "pl",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
