{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mY6EPINerx1l"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_circles\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k71HZUgGtepH",
    "lang": "pl"
   },
   "source": [
    "# Metoda eXtreme Gradient Boosting (XGBoost)\n",
    "\n",
    "Obok sieci neuronowych lider w konkursach keggla [keggle.com](keggle.com). W odróżnieniu od sieci neuronowych najlepiej sprawdza się w przypadku danych strukturalnych (zebranych w tabelach).\n",
    " \n",
    "Za dokumentacją SPSS:   XGBoost Tree to zaawansowana implementacja algorytmu wzmacniania gradientowego (boosting), Modelem bazowym dla Gradient Boostingu jest  drzewo decyzyjne. W kolejnych iteracjach algorytmu tworzone są drzewa, \n",
    "dopasowujące sie do błędu resztkowego popełnionego przez poprzedni predyktor. Ostateczny wynik jest sumą modeli składowych. Dodatkowo warto zauważyć, że każde kolejne drzewo podobnie jak w metodzie lasów losowych budowane jest w oparciu o wylosowany podzbiór zmiennych i obserwacji.\n",
    "\n",
    "W tej metodzie drzewa klasyfikacyjne łączymy **szeregowo**\n",
    "\n",
    "\n",
    "\n",
    "![alt text](https://luckytoilet.files.wordpress.com/2018/01/11.png?w=984)\n",
    "\n",
    "# Idea  gradient boostingu\n",
    "## Kroki algorytmu\n",
    "1. Ustal liczbę modeli bazowych $M$\n",
    "2. Przyjmij równe początkowe wagi dla obserwacji ze zbioru uczącego $U$\n",
    "3. Dla $m = 1, \\dots , M$ wykonaj następujące kroki:\n",
    "    \n",
    "    a) Stwórz model bazowy\n",
    "    \n",
    "    b) Oblicz **residua** (czyli wartość rzeczywista minus wartość przewidziana przez model)\n",
    "    $$ e_1= \\hat{y} - y_{predicted1}$$\n",
    "    \n",
    "    c) Wytrenuj nowy model z otrzymanymi  residuami jako y_train. Oblicz $e_{predicted1}$\n",
    "    \n",
    "    d) Dodaj residua przewidziane przez nowy model do wyniku poprzedniego modelu\n",
    "    \n",
    "    $$ y_{predicted2}=  y_{predicted1}+ e_{predicted1}$$\n",
    "    \n",
    "    e) Oblicz nowe residua i wytrenuj kolejny model .....\n",
    "    \n",
    "\n",
    "### Ciekawoskta:\n",
    "Pierwotnie powstała idea boostingu. Dopiero później pokazano, że takie podejście jest równoznaczne z minimalizacją gradientu pewnej funkcji kosztu. W przypadku regresji można zauważyć, że poszukiwanie modelu o zerowych residuach, zdefiniowanych jako:\n",
    "$$r =  \\sum (\\hat{y}-y_{predicted})$$\n",
    "\n",
    "jest równoznaczne z poszukiawniem minimum funkcji:\n",
    "$$ \\mathcal{L} = \\sum (\\hat{y}-y_{predicted})^2$$\n",
    "\n",
    "$$\\frac{d\\mathcal{L} }{d y_{predicted}}  = - 2 \\sum (\\hat{y}-y_{predicted}) $$\n",
    "\n",
    "\n",
    "## Wróćmy do irysów..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# EXtreme Gradient Boosting method (XGBoost)\n",
    "\n",
    "Next to neural networks, the XGBoost metis hod the leader in keggla competitions [keggle.com](keggle.com). Unlike neural networks, it works best for structured data (collected in tables).\n",
    "  \n",
    "Behind the SPSS documentation: XGBoost Tree is an advanced implementation of the boosting algorithm. The base model for Gradient Boosting is the decision tree. In subsequent iterations of the algorithm, trees are created,\n",
    "matching the residual error made by the previous predictor. The final result is the sum of the component. In addition, it is worth noting that each subsequent tree, similarly to the random forest method, is built based on a random subset of variables and observations.\n",
    "\n",
    "In this method, we classify trees **in series**\n",
    "\n",
    "\n",
    "\n",
    "! [alt text] (https://luckytoilet.files.wordpress.com/2018/01/11.png?w=984)\n",
    "\n",
    "# Idea of gradient boosting\n",
    "## Algorithm steps\n",
    "1. Determine the number of $M$ base models\n",
    "2. Take equal initial weights for observations from the $U$ learning set\n",
    "3. For $m = 1, \\dots , M$, follow these steps:\n",
    "    \n",
    "    a) Create a base model\n",
    "    \n",
    "    b) Calculate **residua** (i.e. actual value minus the value predicted by the model)\n",
    "    $$ e_1= \\hat{y} - y_{predicted1}$$\n",
    "    \n",
    "    c) Train a new model with the residues obtained as y_train. Calculate $e_{predicted1}$\n",
    "    \n",
    "    d) Add the residues provided by the new model to the result of the previous model\n",
    "    \n",
    "    $$ y_{predicted2}=  y_{predicted1}+ e_{predicted1}$$\n",
    "    \n",
    "    e) Calculate new residues and train another model .....\n",
    "    \n",
    "\n",
    "### Mathematical property:\n",
    "\n",
    "The idea of  boosting preceded the mathematical description of this method. It was  later shown that boosting approach minimize the gradient of a certain cost function. In the case of regression, it can be seen that the search for a model with zero residues, defined as:\n",
    "$$r =  \\sum (\\hat{y}-y_{predicted})$$\n",
    "\n",
    "is the same as searching mini the minimum of the function:\n",
    "$$ \\mathcal{L} = \\sum (\\hat{y}-y_{predicted})^2$$\n",
    "\n",
    "$$\\frac{d\\mathcal{L} }{d y_{predicted}}  = - 2 \\sum (\\hat{y}-y_{predicted}) $$\n",
    "\n",
    "\n",
    "## Let's go back to irises ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "si4tgoqYr65p"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5Q2XUjptM8s"
   },
   "outputs": [],
   "source": [
    "clf = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "0RUo6T0FwWAU",
    "outputId": "6b03eba5-cbfc-48d0-cc40-9d96fd85916f"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "94wWq-_Wwafk",
    "outputId": "707b55af-f976-47af-9e68-6304e2d128ab"
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bL4POO4IxmWt"
   },
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], \n",
    "              'learning_rate': [0.01,0.05, 0.1], \n",
    "              'max_depth': [3,6, 10], \n",
    "              'n_estimators': [3, 10, 50, 100, 200]\n",
    "              }\n",
    "\n",
    "model = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QazUGRxTwpHM"
   },
   "outputs": [],
   "source": [
    "clf_grid = GridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "C6Lxhsy4yQLX",
    "outputId": "d6a1355d-0ca7-49da-844f-7c6a6c861901"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "clf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "iP65AELzwpJW",
    "outputId": "cef48a67-e8f9-4113-8ecb-7a12971772af"
   },
   "outputs": [],
   "source": [
    "print (\"The best accuracy on training set  {}\".format(clf_grid.best_score_))\n",
    "print (\"The best parameters: {}\".format(clf_grid.best_params_))\n",
    "print (\"The model accuracy on test set  {}\".format(clf_grid.best_estimator_.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW3wVQSO3snY",
    "lang": "pl"
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "Zobacz jaką dokładność modelu jesteś w stanie otrzymać dla danych `breast_cancer`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Task\n",
    "\n",
    "Check what model accuracy you can get for `breast_cancer` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai6ynbGywkPR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "XGBoost.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
