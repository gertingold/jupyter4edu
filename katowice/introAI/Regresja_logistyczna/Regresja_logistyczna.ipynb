{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6807741c0de771a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Regresja logistyczna (1d) metodą najmniejszego spadku\n",
    "\n",
    "Rozważmy  klasyfikator liniowy w postaci:\n",
    "\n",
    "$$ y = \\sigma ( x\\cdot w + b) $$\n",
    "\n",
    "$x$ jest wektorem cech, w ogólnym przypadku mającym $n$ składowych.\n",
    "\n",
    "Funkcja nieliniowa $\\sigma(x) $ jest dana wzrorem:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}. $$\n",
    "\n",
    "Zauważ, że jej pochodna wynosi:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\left(1-\\sigma(x)\\right).$$\n",
    "\n",
    "\n",
    "Zaimplementujemy algorytm najmniejszego spadku w przypadku **jednowymiarowym** dla regresji liniowej. Weżmy następujące dane:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Logistic regression (1d) by the steepest descent\n",
    "Consider a linear classifier in the form:\n",
    "\n",
    "$$ y = \\sigma ( x\\cdot w + b) $$\n",
    "\n",
    "$x$ is a feature vector, in general having $n$ components.\n",
    "\n",
    "The $\\sigma(x) $ nonlinear function is given by the eye:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}. $$\n",
    "\n",
    "Note that its derivative is:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x) \\left(1-\\sigma(x)\\right).$$\n",
    "\n",
    "\n",
    "We implement the algorithm of the smallest decrease in the case of **one-dimensional** for linear regression. Let's take the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac6d0b5a01ac4856",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.array([-2.29399323, -1.43363036, -0.52468804, -0.39544295, -0.24097318,\n",
    "       -0.14898657, -0.0343155 ,  0.19022609,  0.38726758,  0.59190507,\n",
    "        0.91906829,  1.03690893])\n",
    "\n",
    "y= np.array([-7.77733551, -4.70740336, -2.41251556,  0.36564371, -1.9492586 ,\n",
    "       -0.19388007,  0.82003484,  2.6322221 ,  2.26459065,  2.97531505,\n",
    "        4.8613992 ,  7.20417432])\n",
    "m = x.shape[0]\n",
    "y[y>0] = 1\n",
    "y[y<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c17f3b391351b636",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w, b = 1, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b10d88cc56e3bcb9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sigmoid = lambda x:1/(1+np.exp(-x))\n",
    "sigmoid_p = lambda x:sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4d2ab73728b1fe5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'b.')\n",
    "xlin = np.linspace(-2,2,55)\n",
    "plt.plot(xlin,sigmoid(w*xlin+b),'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ce6d5b7367bb11f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 1 - Funkcja straty\n",
    "\n",
    "Niech funkcją straty będzie suma kwadratów odchyleń przewidywania modelu od rzeczywistej wartości:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2$$\n",
    "\n",
    "Zaimplemenuj funkcję straty. \n",
    "\n",
    "*Uwaga* - zakładamy, że `x` I `y` są globalnymi zmiennymi zainicjowanymi w komórkach powyżej.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 1 - Loss function\n",
    "\n",
    "Let the loss function be the sum of squares of deviations of the model prediction from the real value:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2$$\n",
    "\n",
    "Implement the loss function.\n",
    "\n",
    "*Note* - we assume that `x` and` y` are global variables initialized in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d769269946a18db",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "L = None\n",
    "### BEGIN SOLUTION\n",
    "L = lambda w,b: 0.5/x.shape[0]*np.sum( (sigmoid(w*x+b)-y)**2 )\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-da2800de6373381a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal(L(1,3),0.15719671687181994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-826dab8dd210b045",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "### Zadanie 2 - zaimplementuj pochodne (gradienty)\n",
    "\n",
    "Oblicz pochodne funkcji  strat po parametrach w i b\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial  \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2}{\\partial w}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i) \\sigma'(w x_i+b) x_i\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial  \\frac{1}{2} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2}{\\partial b}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)  \\sigma'(w x_i+b)\n",
    "$$\n",
    "\n",
    "w punkcie $w = 1$, $b = 0.1$.\n",
    "\n",
    "Oznaczenie `dw`  użyte w kodzie poniżej często się stosuje jako skrót $\\frac{\\partial L}{\\partial w}$. Podobnie  jest z `db`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 2 - implement derivatives (gradients)\n",
    "\n",
    "Calculate the derivatives of the loss function after the parameters w and b\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial  \\frac{1}{2m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2}{\\partial w}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i) \\sigma'(w x_i+b) x_i\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial  \\frac{1}{2} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)^2}{\\partial b}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (\\sigma(w x_i+b) - y_i)  \\sigma'(w x_i+b)\n",
    "$$\n",
    "\n",
    "at $w = 1$, $b = 0.1$.\n",
    "\n",
    "The designation `dw` used in the code below is often used as the abbreviation $\\frac{\\partial L}{\\partial w}$. It's similar with `db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5de99454bf9d18ea",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dw, db = None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dw = 1/m*(sigmoid(w*x+b)-y).dot(sigmoid_p(w*x+b)*x)\n",
    "db = 1/m*(sigmoid(w*x+b)-y).dot(sigmoid_p(w*x+b))\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-69ca8a96ae252c1c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal(-0.04698730835958943, dw,significant=6)\n",
    "np.testing.assert_approx_equal(0.03527485740880648 ,db,significant=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a711726dfeaab19",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 3\n",
    "\n",
    "Oblicz pochodne korzystając ze wzoru na iloraz różnicowy:\n",
    "\n",
    "$$\\frac{df}{dx} \\simeq \\frac{f(x+h)-f(x)}{h}$$ \n",
    "\n",
    "Przyjmij $h=0.001$ (ważne aby wyszły testy).\n",
    "\n",
    "Czy otrzymane wartości są zbliżone do anliczonych analitycznie? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Calculate derivatives using the formula for differential quotient:\n",
    "\n",
    "$$\\frac{df}{dx} \\simeq \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "Take $h=0.001$ (it's important that the tests come out).\n",
    "\n",
    "Are the obtained values ​​close to analytically calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8e31777ac8f4acd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dw_num, db_num = None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dw_num = (L(w+0.001,b)-L(w,b))/0.001\n",
    "db_num = (L(w,b+0.001)-L(w,b))/0.001\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-52d65c48403302a2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal(-0.04699303, dw_num,significant=6)\n",
    "np.testing.assert_approx_equal(0.03526763 , db_num,significant=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8bd98ef0ec8b7a4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 4\n",
    "\n",
    "Zaimplementuj algorytm najmniejszego spadku. W tym celu startując z $w=1$ i $b=0.1$, wykonaj:\n",
    "\n",
    "1. Oblicz gradienty (pochodne) w punktcie $w,b$ korzystając z implementacji `dw` i `db`.\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "\n",
    "Wykonaj 100 takich  kroków z $\\alpha=1.0$. Na wykresie zobaczysz efekt takiego działania.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 4\n",
    "\n",
    "Implement the smallest fall algorithm. To do this, starting with $w=1$ and $b=0.1$, do:\n",
    "\n",
    "1. Calculate the gradients (derivatives) at the $w,b$ point using the implementation of `dw` and` db`.\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "\n",
    "Follow 100 such steps with $\\alpha=1.0$. The graph will show the effect of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bcff12dc01b1005e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "w, b = 1., 3.\n",
    "for i in range(100):\n",
    "    pass  \n",
    "### BEGIN SOLUTION\n",
    "    \n",
    "    dw = 1/m*(sigmoid(w*x+b)-y).dot(sigmoid_p(w*x+b)*x)\n",
    "    db = 1/m*(sigmoid(w*x+b)-y).dot(sigmoid_p(w*x+b))\n",
    "    \n",
    "    w = w - alpha*dw\n",
    "    b = b - alpha*db\n",
    "### END SOLUTION\n",
    "    \n",
    "print(L(w,b),w,b,dw,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca4c968fd3208df7",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal( w,2.601,  significant=3)\n",
    "np.testing.assert_approx_equal( b, 0.7189, significant=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'b.')\n",
    "xlin = np.linspace(-2,2,55)\n",
    "plt.plot(xlin,sigmoid(w*xlin+b),'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "pl",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
