{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c79d45b5f2e42f1f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Neuron\n",
    "\n",
    "[Sieci neuronowe](https://pl.wikipedia.org/wiki/Sie%C4%87_neuronowa) składają się z neuronów połączonych ze sobą. W najprostszym przypadku każdy neuron może mieć wiele wejść, ale tylko jedno wyjście. Ponadto każdy neuron posiada wektor wag (jedna waga na każde wejście) oraz bias, który jest liczbą.\n",
    "Wagi należy początkowo zainicjalizować do losowych wartości. Najczęściej losuje się je z rozkładu normalnego o zerowej średniej. Z kolei początkowy bias można przyjąć w zasadzie dowolny. Zazwyczaj wybiera się losowo jakąś niewielką liczbę dodatnią np. z przedziału $[0,0.1]$\n",
    "\n",
    "Do zaimplementowania neuronu wystarczą 3 podstawowe funkcje:\n",
    "\n",
    "Forward pass - propagujemy dane od wejścia neuronu do wyjścia\n",
    "\n",
    "Backward pass - propagujemy gradient od wyjścia do wejścia, oraz wyliczamy gradienty wag\n",
    "\n",
    "Parameter update - etap w którym zmieniamy wagi neuronu (etap uczenia neuronu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Neuron\n",
    "\n",
    "[Neural networks] (https://en.wikipedia.org/wiki/Artificial_neural_network) consist of neurons connected to each other. In the simplest case, each neuron can have many inputs, but only one output. In addition, each neuron has a weight vector (one weight for each input) and bias, which is a number.\n",
    "Initially, weights should be initialized to random values. They are most often drawn from the normal distribution with zero mean. In turn, the initial bias can be taken basically any. Usually a small positive number is randomly selected, e.g. from $[0,0.1]$\n",
    "\n",
    "To implement a neuron, 3 basic functions are sufficient:\n",
    "\n",
    "Forward pass - we propagate data from neuron input to output\n",
    "\n",
    "Backward pass - we propagate the gradient from output to input, and calculate weight gradients\n",
    "\n",
    "Parameter update - a stage where we change the neuron weights (neuron learning stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0ccb98cada720e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c3cd027f0743a7a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self, numberOfInputs):\n",
    "        self.weights = None\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        self.tmp_input = None\n",
    "        self.tmp_gradient = 0.0\n",
    "        self.tmp_output = 0.0\n",
    "        ### BEGIN SOLUTION\n",
    "        self.weights = np.random.randn(numberOfInputs) / np.sqrt(numberOfInputs)\n",
    "        self.bias = 0.1 * np.random.uniform()\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward(self, inputVector):\n",
    "        ### BEGIN SOLUTION\n",
    "        output = float(np.dot(self.weights , inputVector) + self.bias)\n",
    "        output = 1.0 / (1.0 + np.exp(-output))\n",
    "        self.tmp_input = np.copy(inputVector)\n",
    "        self.tmp_output = output\n",
    "        return output\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def backward(self, gradient):\n",
    "        ### BEGIN SOLUTION\n",
    "        activation_gradient = self.tmp_output * (1.0 - self.tmp_output)\n",
    "        self.tmp_gradient = gradient * activation_gradient\n",
    "        output = self.tmp_gradient * self.weights\n",
    "        return output\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def learn(self, learningRate):\n",
    "        ### BEGIN SOLUTION\n",
    "        weight_update = self.tmp_input * self.tmp_gradient\n",
    "        bias_update = self.tmp_gradient\n",
    "        \n",
    "        # SGD\n",
    "        self.weights += learningRate * weight_update\n",
    "        self.bias += learningRate * bias_update\n",
    "        ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b6af7e03bf6dc4d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Forward pass\n",
    "\n",
    "Każdy neuron wylicza swoją aktywację (wartość na wyjściu) w następujący sposób.\n",
    "\n",
    "$$ y_{out}=f_{act}(\\overline{w} \\cdot \\overline{x_{in}}+b)$$\n",
    "\n",
    "gdzie $\\cdot$ to iloczyn skalarny, $\\overline{w}$ to wektor wag neuronu, $\\overline{x_{in}}$ to wektor danych wejściowych, $b$ to bias, a $f_{act}$ to pewna nieliniowa funkcja aktywacyjna. Można wybrać wiele różnych takich funkcji, w tym zestawie użyjemy funkcji sigmoidalnej\n",
    "$$f_{act}(x)=sigm(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Należy zwrócić uwagę, żeby wartość zwracana przez funkcję $forward$ była liczbą. Ponadto należy skopiować wektor wejściowy do tmp_input, oraz zapamiętać wynik w tmp_output. Będzie to potrzebne w następnych zadaniach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Forward pass\n",
    "\n",
    "Each neuron calculates its activation (output value) as follows.\n",
    "\n",
    "$$ y_{out}=f_{act}(\\overline{w} \\cdot \\overline{x_{in}}+b)$$\n",
    "\n",
    "where $\\cdot$ is a scalar product, $\\overline{w}$ is a neuron weights vector, $\\overline{x_{in}}$ is an input vector, $b$ is a bias, and $f_{act}$ is a nonlinear activation function. You can choose many different such functions, in this set we will use the sigmoidal function\n",
    "$$f_{act}(x)=sigm(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Note that the value returned by $forward$ should be a number. In addition, copy the input vector to tmp_input, and save the result in tmp_output. You will need it in the next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b2ad707969a15ba6",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Neuron(4)\n",
    "n.weights = np.array((0.1, -0.2, 0.3, -0.1))\n",
    "n.bias = 1.0\n",
    "\n",
    "inp = np.array((1, 2, 3, -4))\n",
    "out = n.forward(inp)\n",
    "\n",
    "np.testing.assert_almost_equal(out, 1.0 / (1.0 + np.exp(-2)))\n",
    "np.testing.assert_equal(type(out), np.float64)\n",
    "np.testing.assert_equal(n.tmp_input, inp)\n",
    "np.testing.assert_equal(n.tmp_output, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-76073396ae4abdda",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Backward pass\n",
    "\n",
    "W tej części [propagujemy gradient wstecz](https://en.wikipedia.org/wiki/Backpropagation) według następującego wzoru\n",
    "\n",
    "$$ \\overline{g}_{out}=g_{in} f_{act}' \\overline{w}$$\n",
    "\n",
    "Gdzie $g_{in}$ to gradient który dociera do wyjścia neuronu (argument funkcji $backward$), $\\overline{w}$ to wektor wag neuronu, a $f_{act}'$ to pochodna funkcji aktywacyjnej. Funkcja sigmoidalna ma przydatną własność $sigm(x)'=sigm(x)*(1-sigm(x))$, w której należy wykorzystać zapamiętaną wartość $y_{out}$.\n",
    "\n",
    "Przy okazji propagacji wstecz można wyliczyć gadienty ze względu na wagi oraz bias.\n",
    "$$ \\Delta \\overline{w}=g_{in} f_{act}' \\overline{x}_{in}$$\n",
    "$$ \\Delta b=g_{in} f_{act}' $$\n",
    "Przydaje się tutaj zapamiętana wartość tmp_input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Backward pass\n",
    "\n",
    "In this part [we propagate a gradient back] (https://en.wikipedia.org/wiki/Backpropagation) according to the following formula\n",
    "\n",
    "$$ \\overline{g}_{out}=g_{in} f_{act}' \\overline{w}$$\n",
    "\n",
    "Where $g_{in}$ is the gradient that reaches the neuron output (argument of the $backward$ function), $\\overline{w}$ is the neuron weight vector, and $f_{act}'$ is the derivative of the activation function. The sigmoid function has the useful property of $sigm(x)'=sigm(x)*(1-sigm(x))$, in which the stored value of $y_{out}$ should be used.\n",
    "\n",
    "During back propagation, you can calculate gradients by weight and bias.\n",
    "$$ \\Delta \\overline{w}=g_{in} f_{act}' \\overline{x}_{in}$$\n",
    "$$ \\Delta b=g_{in} f_{act}' $$\n",
    "The stored tmp_input value is useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2f30c19f599c8cde",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Neuron(4)\n",
    "n.weights = np.array((0.1, -0.2, 0.3, -0.1))\n",
    "n.bias = 1.0\n",
    "\n",
    "inp = np.array((1, 2, 3, -4))\n",
    "out = n.forward(inp)\n",
    "back = n.backward(0.4)\n",
    "v = 0.4 * out * (1 - out) * n.weights\n",
    "np.testing.assert_almost_equal(back, v)\n",
    "np.testing.assert_equal(type(back), np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b46fea749ffabf59",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Update\n",
    "\n",
    "Mając gradienty ze względu na wagi i bias $ \\Delta \\overline{w}$, $ \\Delta b$ można wyliczyć ich nowe wartości, które nieco zmniejszą błąd.\n",
    "\n",
    "$$\\overline{w}_{i+1}=\\overline{w}_{i}+\\alpha  \\Delta \\overline{w}$$\n",
    "$$b_{i+1}=b_{i}+\\alpha  \\Delta b$$\n",
    "\n",
    "Powyższe wzory odpowiadają metodzie SGD z poprzedniego zestawu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Update\n",
    "\n",
    "Given the $ \\Delta \\overline{w}$ weight and bias gradients, $ \\Delta b$ can calculate their new values ​​that will slightly reduce the error.\n",
    "\n",
    "$$\\overline{w}_{i+1}=\\overline{w}_{i}+\\alpha  \\Delta \\overline{w}$$\n",
    "$$b_{i+1}=b_{i}+\\alpha  \\Delta b$$\n",
    "\n",
    "The above formulas correspond to the SGD method from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-531c194509768d76",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Neuron(1)\n",
    "\n",
    "for i in range(20000):\n",
    "    inp = np.array((2 * np.random.randint(0, 2) - 1))   \n",
    "    out = n.forward(inp)\n",
    "    back = n.backward(1.0/(1.0 + np.exp(1 - 2 * inp)) - out)\n",
    "    n.learn(0.1)\n",
    "\n",
    "np.testing.assert_equal(int(round(100 * n.weights[0])), 200)\n",
    "np.testing.assert_equal(int(round(100 * n.bias)), -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
