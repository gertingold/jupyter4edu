{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa34f99ebe1bfecc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a81f44f2ed8721b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Sieć neuronowa\n",
    "\n",
    "[Sieci neuronowe](https://pl.wikipedia.org/wiki/Sie%C4%87_neuronowa) składają się z warstw neuronów połączonych ze sobą. Każda warstwa składa się z neuronów takich jak implementowaliśmy w poprzednim zestawie, jednak wydajniej jest ją zaimplementować jako całość niż składać z pojedynczych obiektów. Warstwa neuronów posiada macierz wag $W$ oraz wektor biasów $\\overline{b}$.\n",
    "Wagi należy początkowo zainicjalizować do losowych wartości. Najczęściej losuje się je z rozkładu normalnego o zerowej średniej i jednostkowej wariancji. Z kolei początkowe biasy można przyjąć w zasadzie dowolne (mogą to być małe liczby losowe).\n",
    "\n",
    "Do zaimplementowania warstwy wystarczą 3 podstawowe funkcje:\n",
    "\n",
    "Forward pass - propagujemy dane od wejścia warstwy do wyjścia\n",
    "\n",
    "Backward pass - propagujemy gradient od wyjścia do wejścia\n",
    "\n",
    "Parameter update - etap w którym wyliczamy gradienty wag i biasów (etap uczenia neuronów)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Neuron network\n",
    "\n",
    "[Neural networks] (https://en.wikipedia.org/wiki/Artificial_neural_network) consist of layers of neurons connected to each other. Each layer consists of neurons as we implemented in the previous set, but it is more efficient to implement it as a whole than to assemble it from individual objects. The neuron layer has the $W$ weight matrix and the $\\overline{b}$ bias vector.\n",
    "Initially, weights should be initialized to random values. They are most often drawn from the normal distribution with zero mean and unit variance. In turn, the initial bias can be assumed to be basically any value(they can be small random numbers).\n",
    "\n",
    "To implement the layer, 3 basic functions are sufficient:\n",
    "\n",
    "Forward pass - we propagate data from layer entry to exit\n",
    "\n",
    "Backward pass - we propagate the gradient from exit to entry\n",
    "\n",
    "Parameter update - a stage where we calculate weight and bias gradients (neuron learning step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7d45e47e0622092",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self, numberOfNeurons, numberOfInputs):\n",
    "        self.numberOfInputs = numberOfInputs\n",
    "        self.numberOfNeurons = numberOfNeurons\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "        self.tmp_input = None\n",
    "        self.tmp_gradient = None\n",
    "        self.tmp_output = None\n",
    "        ### BEGIN SOLUTION\n",
    "        self.weights = np.random.randn(numberOfNeurons, numberOfInputs) / np.sqrt(numberOfInputs)\n",
    "        self.bias = np.random.uniform(0, 0.1, numberOfNeurons)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def forward(self, inputVector):\n",
    "        ### BEGIN SOLUTION\n",
    "        self.tmp_output = np.dot(self.weights, inputVector) + self.bias\n",
    "        self.tmp_output = 1.0 / (1.0 + np.exp(-self.tmp_output))\n",
    "        self.tmp_input = inputVector\n",
    "        return self.tmp_output\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def backward(self, gradient):\n",
    "        ### BEGIN SOLUTION\n",
    "        activation_gradient = self.tmp_output * (1.0 - self.tmp_output)\n",
    "        self.tmp_gradient = gradient * activation_gradient\n",
    "        output = np.dot(np.transpose(self.weights), self.tmp_gradient)\n",
    "        return output\n",
    "        ### END SOLUTION\n",
    "        \n",
    "    def learn(self, learningRate):\n",
    "        ### BEGIN SOLUTION\n",
    "        weight_update = np.tensordot(self.tmp_gradient, self.tmp_input, axes=0)\n",
    "        bias_update = self.tmp_gradient\n",
    "        \n",
    "        self.weights += learningRate * weight_update\n",
    "        self.bias += learningRate * bias_update\n",
    "        ### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a5d736f4bdf272f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Forward pass\n",
    "\n",
    "Warstwa neuronów wylicza swoją aktywację (wektor na wyjściu) w następujący sposób.\n",
    "\n",
    "$$ \\overline{y}_{out}=f_{act}(W \\cdot \\overline{x}_{in}+\\overline{b})$$\n",
    "\n",
    "gdzie $\\cdot$ to iloczyn skalarny, $W$ to macierz wag neuronu, $\\overline{x}_{in}$ to wektor danych wejściowych, $\\overline{b}$ to wektor biasów, a $f_{act}$ to pewna nieliniowa funkcja aktywacyjna. Można wybrać wiele różnych takich funkcji, w tym zestawie także użyjemy funkcji sigmoidalnej\n",
    "$$f_{act}(x)=sigm(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Należy zwrócić uwagę, żeby funkcja $forward$ zwracała np.array. Ponadto należy skopiować wektor wejściowy do self.tmp_input, oraz zapamiętać wynik w self.tmp_output. Będzie to potrzebne w następnych zadaniach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Forward pass\n",
    "\n",
    "The neuron layer calculates its activation (output vector) as follows.\n",
    "\n",
    "$$ \\overline{y}_{out}=f_{act}(W \\cdot \\overline{x}_{in}+\\overline{b})$$\n",
    "\n",
    "where $\\cdot$ is a scalar product, $W$ is a neuron weight matrix, $\\overline{x}_{in}$ is an input vector, $\\overline{b}$ is a bias vector, and $f_{act}$ is a nonlinear activation function. You can choose many different such functions, and in this set we will also use a sigmoidal function\n",
    "$$f_{act}(x)=sigm(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Make sure $forward$ returns np.array. In addition, copy the input vector to self.tmp_input, and save the result in self.tmp_output. You will need it in the next tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-49e37dc8b710cce7",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Layer(7, 4)\n",
    "n.weights = np.reshape(np.arange(0, 2.8, 0.1), (7, 4))\n",
    "n.bias = np.array(np.arange(-0.3, 0.4, 0.1))\n",
    "\n",
    "inp = np.array((1, 2, 3, -4))\n",
    "out = n.forward(inp)\n",
    "\n",
    "np.testing.assert_array_almost_equal(out, np.array([0.3318, 0.5498, 0.7502, 0.8807, 0.9478, 0.9781, 0.9909]), decimal=3)\n",
    "np.testing.assert_equal(type(out), np.ndarray)\n",
    "np.testing.assert_equal(type(out[0]), np.float64)\n",
    "np.testing.assert_equal(out.shape, (7,))\n",
    "np.testing.assert_equal(n.tmp_input, inp)\n",
    "np.testing.assert_equal(n.tmp_output, out)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "n.weights = np.random.rand(7, 4)\n",
    "n.bias = np.random.rand(7)\n",
    "inp = np.random.rand(4)\n",
    "out = n.forward(inp)\n",
    "correct = 1.0 / (1.0 + np.exp(-np.dot(n.weights, inp) - n.bias))\n",
    "np.testing.assert_array_almost_equal(out, correct, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d6bd119da003ea7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Backward pass\n",
    "\n",
    "W tej części [propagujemy gradient wstecz](https://en.wikipedia.org/wiki/Backpropagation) według następującego wzoru\n",
    "\n",
    "$$ \\overline{g}_{out}=W^T \\cdot(\\overline{g}_{in} \\odot \\overline{f'}_{act})$$\n",
    "\n",
    "Gdzie $\\overline{g}_{in}$ to gradient który dociera do warstwy (argument funkcji $backward$), $W$ to macierz wag, a $\\overline{f'}_{act}$ to wektor pochodnych funkcji aktywacyjnych. Operator $\\odot$ to iloczyn Hadamarda. Funkcja sigmoidalna ma przydatną własność $sigm(x)'=sigm(x)*(1-sigm(x))$, w której należy wykorzystać zapamiętaną wartość $\\overline{y}_{out}$. Wynik $\\overline{g}_{in} \\odot \\overline{f'}_{act}$ należy zapamiętać jako self.tmp_gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Backward pass\n",
    "\n",
    "In this part [we propagate a gradient back] (https://en.wikipedia.org/wiki/Backpropagation) according to the following formula\n",
    "\n",
    "$$ \\overline{g}_{out}=W^T \\cdot(\\overline{g}_{in} \\odot \\overline{f'}_{act})$$\n",
    "\n",
    "Where $\\overline{g}_{in}$ is the gradient that reaches the layer ($backward$ function argument), $W$ is the weight matrix, and $\\overline{f'}_{act}$ is the vector of derived activation functions. The $\\odot$ operator is the product of Hadamard. The sigmoid function has the useful property of $sigm(x)'=sigm(x)*(1-sigm(x))$, in which the stored value of $\\overline{y}_{out}$ should be used. The $\\overline{g}_{in} \\odot \\overline{f'}_{act}$ result should be saved as self.tmp_gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0daebe4934725503",
     "locked": true,
     "points": 6,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Layer(7, 4)\n",
    "n.weights = np.reshape(np.arange(0, 2.8, 0.1), (7, 4))\n",
    "n.bias = np.array(np.arange(-0.3, 0.4, 0.1))\n",
    "n.tmp_output = np.array([0.3318, 0.5498, 0.7502, 0.8807, 0.9478, 0.9781, 0.9909])\n",
    "\n",
    "grad = np.array((-4, 2, 1, -2, 1, 0, -1))\n",
    "prev = n.backward(grad)\n",
    "\n",
    "np.testing.assert_array_almost_equal(prev, np.array([0.153, 0.116, 0.078, 0.041]), decimal=3)\n",
    "np.testing.assert_equal(type(prev), np.ndarray)\n",
    "np.testing.assert_equal(type(prev[0]), np.float64)\n",
    "np.testing.assert_equal(prev.shape, (4,))\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "n.weights = np.random.rand(7, 4)\n",
    "n.bias = np.random.rand(7)\n",
    "inp = np.random.rand(4)\n",
    "grad = np.random.rand(7)\n",
    "out = n.forward(inp)\n",
    "prev = n.backward(grad)\n",
    "\n",
    "correct = np.dot(n.weights.T, grad * (1.0 - out) * out)\n",
    "np.testing.assert_array_almost_equal(prev, correct, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1e43b90b4544370b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Update\n",
    "\n",
    "Gradienty ze względu na wagi oraz bias:\n",
    "$$ \\Delta W = (\\overline{g}_{in} \\odot \\overline{f'}_{act}) \\cdot \\overline{x}_{in}^T$$\n",
    "$$ \\Delta \\overline{b} = \\overline{g}_{in} \\odot \\overline{f'}_{act} $$\n",
    "Przydaje się tutaj zapamiętana wartość self.tmp_input oraz self.tmp_gradient\n",
    "\n",
    "Mając gradienty ze względu na wagi i bias $ \\Delta \\overline{w}$, $ \\Delta b$ można wyliczyć ich nowe wartości, które nieco zmniejszą błąd.\n",
    "\n",
    "$$W_{i+1} = W_{i}+\\alpha  \\Delta W$$\n",
    "$$\\overline{b}_{i+1} = \\overline{b}_{i}+\\alpha  \\Delta \\overline{b}$$\n",
    "\n",
    "Powyższe wzory odpowiadają metodzie SGD z poprzednich zestawów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Update\n",
    "\n",
    "Gradients due to weight and bias:\n",
    "$$ \\Delta W = (\\overline{g}_{in} \\odot \\overline{f'}_{act}) \\cdot \\overline{x}_{in}^T$$\n",
    "$$ \\Delta \\overline{b} = \\overline{g}_{in} \\odot \\overline{f'}_{act} $$\n",
    "The self.tmp_input and self.tmp_gradient values are useful here\n",
    "\n",
    "Given the weight and bias gradients, $ \\Delta \\overline{w}$ and $ \\Delta b$ can calculate their new values that will slightly reduce the error.\n",
    "\n",
    "$$W_{i+1} = W_{i}+\\alpha  \\Delta W$$\n",
    "$$\\overline{b}_{i+1} = \\overline{b}_{i}+\\alpha  \\Delta \\overline{b}$$\n",
    "\n",
    "The above formulas correspond to the SGD method from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d008d8c9ce4bad77",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n = Layer(7, 4)\n",
    "n.weights = np.zeros((7, 4))\n",
    "n.bias = np.zeros(7)\n",
    "n.tmp_output = np.array([0.3318, 0.5498, 0.7502, 0.8807, 0.9478, 0.9781, 0.9909])\n",
    "n.tmp_gradient = np.array([-0.8868, 0.4950, 0.1873, -0.2101, 0.0494, 0.0, -0.0090])\n",
    "n.tmp_input = np.array((1, 2, 3, -4))\n",
    "n.learn(1)\n",
    "\n",
    "correct_delta_w = np.array(\n",
    "[[-0.8868, -1.7736, -2.6604, 3.5472],\n",
    " [ 0.495 ,  0.99  ,  1.485 , -1.98  ],\n",
    " [ 0.1873,  0.3746,  0.5619, -0.7492],\n",
    " [-0.2101, -0.4202, -0.6303,  0.8404],\n",
    " [ 0.0494,  0.0988,  0.1482, -0.1976],\n",
    " [ 0.    ,  0.    ,  0.    , -0.    ],\n",
    " [-0.009 , -0.018 , -0.027 ,  0.036 ]])\n",
    "\n",
    "np.testing.assert_array_almost_equal(n.weights, correct_delta_w, decimal=3)\n",
    "np.testing.assert_array_almost_equal(n.bias, n.tmp_gradient, decimal=3)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "n.weights = np.zeros((7, 4))\n",
    "n.bias = np.zeros(7)\n",
    "n.tmp_gradient = np.random.rand(7)\n",
    "n.tmp_input = np.random.rand(4)\n",
    "n.learn(1)\n",
    "\n",
    "correct_dw = np.outer(n.tmp_gradient, n.tmp_input)\n",
    "np.testing.assert_array_almost_equal(n.weights, correct_dw, decimal=3)\n",
    "np.testing.assert_array_almost_equal(n.bias, n.tmp_gradient, decimal=3)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "nbTranslate": {
   "displayLangs": [
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
