{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mY6EPINerx1l"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_circles\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k71HZUgGtepH"
   },
   "source": [
    "# Metoda eXtreme Gradient Boosting (XGBoost)\n",
    "\n",
    "Obok sieci neuronowych lider w konkursach keggla [keggle.com](keggle.com). W odróżnieniu od sieci neuronowych najlepiej sprawdza się w przypadku danych strukturalnych (zebranych w tabelach).\n",
    " \n",
    "Za dokumentacją SPSS:   XGBoost Tree to zaawansowana implementacja algorytmu wzmacniania gradientowego (boosting), który jako model bazowy wykorzystuje model drzewa decyzyjnego. Algorytmy wzmacniania iteracyjnie ucząc się, wyznaczają słabe klasyfikatory i dodają je do ostatecznego silnego klasyfikatora. Dodatkowo, każde kolejne drzewo podobnie jak w metodzie lasów losowych budowane jest w oparciu o wylosowany podzbiór zmiennych i obserwacji.\n",
    "\n",
    "W tej metodzie drzewa klasyfikacyjne łączymy **szeregowo**\n",
    "\n",
    "W metodzie tej tworzymy wiele, **zróżnicowanych** drzew klasyfikacyjnych, których dokładność musi przekraczać 50%, jednak nie może być też zbyt duża, aby modele były rzeczywiście zróżnicowane.  \n",
    "2. Każdy z tych modeli budujemy jedynie na **`n` wybranych zmiennych**.\n",
    "3. Drzewa te połączone są ze sobą **równolegle**\n",
    "4. Ostateczna klasyfikacja dokonywana jest na drodze głosowania przez poszczególne modele.\n",
    "\n",
    "![alt text](https://luckytoilet.files.wordpress.com/2018/01/11.png?w=984)\n",
    "\n",
    "# Idea  gradient boostingu\n",
    "## Kroki algorytmu\n",
    "1. Ustal liczbę modeli bazowych $M$\n",
    "2. Przyjmij równe początkowe wagi dla obserwacji ze zbioru uczącego $U$\n",
    "3. Dla $m = 1, \\dots , M$ wykonaj następujące kroki:\n",
    "    \n",
    "    a) Stwórz model bazowy\n",
    "    \n",
    "    b) Oblicz **residua** (czyli wartość rzeczywista minus wartość przewidziana przez model)\n",
    "    $$ e_1= \\hat{y} - y_{predicted1}$$\n",
    "    \n",
    "    c) Wytrenuj nowy model z otrzymanymi  residuami jako y_train. Oblicz $e_{predicted1}$\n",
    "    \n",
    "    d) Dodaj residua przewidziane przez nowy model do wyniku poprzedniego modelu\n",
    "    \n",
    "    $$ y_{predicted2}=  y_{predicted1}+ e_{predicted1}$$\n",
    "    \n",
    "    e) Oblicz nowe residua i wytrenuj kolejny model .....\n",
    "    \n",
    "\n",
    "### Ciekawoskta:\n",
    "Pierwotnie powstała idea boostingu. Dopiero później pokazano, że takie podejście jest równoznaczne z minimalizacją gradientu pewnej funkcji kosztu. W przypadku regresji można zauważyć, że poszukiwanie modelu o zerowych residuach, zdefiniowanych jako:\n",
    "$$r =  \\sum (\\hat{y}-y_{predicted})$$\n",
    "\n",
    "jest równoznaczne z poszukiawniem minimum funkcji:\n",
    "$$ \\mathcal{L} = \\sum (\\hat{y}-y_{predicted})^2$$\n",
    "\n",
    "$$\\frac{d\\mathcal{L} }{d y_{predicted}}  = - 2 \\sum (\\hat{y}-y_{predicted}) $$\n",
    "\n",
    "\n",
    "## Wróćmy do irysów..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "si4tgoqYr65p"
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5Q2XUjptM8s"
   },
   "outputs": [],
   "source": [
    "clf = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "0RUo6T0FwWAU",
    "outputId": "6b03eba5-cbfc-48d0-cc40-9d96fd85916f"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "94wWq-_Wwafk",
    "outputId": "707b55af-f976-47af-9e68-6304e2d128ab"
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bL4POO4IxmWt"
   },
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], \n",
    "              'learning_rate': [0.01,0.05, 0.1], \n",
    "              'max_depth': [3,6, 10], \n",
    "              'n_estimators': [3, 10, 50, 100, 200]\n",
    "              }\n",
    "\n",
    "model = xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QazUGRxTwpHM"
   },
   "outputs": [],
   "source": [
    "clf_grid = GridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "C6Lxhsy4yQLX",
    "outputId": "d6a1355d-0ca7-49da-844f-7c6a6c861901"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "clf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "iP65AELzwpJW",
    "outputId": "cef48a67-e8f9-4113-8ecb-7a12971772af"
   },
   "outputs": [],
   "source": [
    "print (\"Najlepsza dokładność na zbiorze treningowym {}\".format(clf_grid.best_score_))\n",
    "print (\"Dobrane parametry: {}\".format(clf_grid.best_params_))\n",
    "print (\"Dokładność na zbiorze testowym {}\".format(clf_grid.best_estimator_.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nW3wVQSO3snY"
   },
   "source": [
    "## Zadanie\n",
    "\n",
    "Zobacz jaką dokładność modelu jesteś w stanie otrzymać dla danych `breast_cancer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai6ynbGywkPR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "XGBoost.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
