{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6807741c0de771a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Regresja liniowa, metoda największego spadku\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Linear regression, the steepest gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac6d0b5a01ac4856",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.array([-2.29399323, -1.43363036, -0.52468804, -0.39544295, -0.24097318,\n",
    "       -0.14898657, -0.0343155 ,  0.19022609,  0.38726758,  0.59190507,\n",
    "        0.91906829,  1.03690893])\n",
    "\n",
    "y= np.array([-7.77733551, -4.70740336, -2.41251556,  0.36564371, -1.9492586 ,\n",
    "       -0.19388007,  0.82003484,  2.6322221 ,  2.26459065,  2.97531505,\n",
    "        4.8613992 ,  7.20417432])\n",
    "m = x.shape[0]\n",
    "# y = 4.2*x + 1.2 + np.random.randn(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c17f3b391351b636",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w, b = 1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4d2ab73728b1fe5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'b.')\n",
    "xlin = np.linspace(-2,2,55)\n",
    "plt.plot(xlin,w*xlin+b,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b5c5e035868e9a96",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Funkcja straty\n",
    "\n",
    "Funkcją straty będzie suma kwadratów odchyleń przewidywania modelu od rzeczywistej wartości:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "The loss function will be the sum of squares of deviations of the model prediction from the actual value:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d769269946a18db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "L = lambda w,b: 0.5/m*np.sum( (w*x+b-y)**2 )\n",
    "L(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e6cb2d86af050eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 1\n",
    "\n",
    "Oblicz pochodne funkcji strat $L(w,b)$ po parametrach $w$ i $b$. \n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial  \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2}{\\partial w}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)  x_i\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial  \\frac{1}{2} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2}{\\partial b}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)  \n",
    "$$\n",
    "\n",
    "w punkcie $w = 1$, $b = 0.1$.\n",
    "\n",
    " - *Uwaga 1:* Oznaczenie `dw`  użyte w kodzie poniżej często się stosuje jako skrót $\\frac{\\partial L}{\\partial w}$. Podobnie  jest z `db`.\n",
    " - *Uwaga 2:* Pochodne określa się potocznie mianem \"gradientów\", ponieważ wagi $w$ w ogólnym przypadku są wektorem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Calculate derivatives of the $L(w,b)$ loss function after the $w$ and $b$ parameters.\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial  \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2}{\\partial w}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)  x_i\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\frac{\\partial  \\frac{1}{2} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2}{\\partial b}  = \n",
    "\\frac{1}{m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)  \n",
    "$$\n",
    "\n",
    "at point $w = 1$, $b = 0.1$.\n",
    "\n",
    " - *Note 1:* The designation `dw` used in the code below is often used as the abbreviation $\\frac{\\partial L}{\\partial w}$. It's similar with `db`.\n",
    " - *Note 2:* Derivatives are commonly referred to as \"gradients\", because the $w$ weights in general are a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5de99454bf9d18ea",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dw, db = None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dw = 1/m*(w*x+b-y).dot(x)\n",
    "db = 1/m*np.sum(w*x+b-y)\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-69ca8a96ae252c1c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal(-2.5971538397574383, dw,significant=6)\n",
    "np.testing.assert_approx_equal(-0.4024700533333332 ,db,significant=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Zadanie 2\n",
    "\n",
    "**Sprawdzanie gradientów**\n",
    "\n",
    "Oblicz pochodne korzystając ze wzoru na iloraz różnicowy:\n",
    "\n",
    "$$\\frac{df}{dx} \\simeq \\frac{f(x+h)-f(x)}{h}$$ \n",
    "\n",
    "Przyjmij $h=0.001$ (jest to niezbędne, aby wyszły testy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "**Checking gradients**\n",
    "\n",
    "Calculate derivatives using the formula for difference quotient:\n",
    "\n",
    "$$\\frac{df}{dx} \\simeq \\frac{f(x+h)-f(x)}{h}$$\n",
    "\n",
    "Take $h=0.001$ (it is necessary for the tests to come out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8e31777ac8f4acd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dw_num, db_num = None, None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "dw_num = (L(w+0.001,b)-L(w,b))/0.001\n",
    "db_num = (L(w,b+0.001)-L(w,b))/0.001\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-52d65c48403302a2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal( -2.5967252065637325,dw_num,significant=6)\n",
    "np.testing.assert_approx_equal(-0.4019700533337556 ,db_num,significant=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8bd98ef0ec8b7a4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 3\n",
    "\n",
    "Zaimplementuj algorytm najmniejszego spadku. W tym celu startując z $w=1$ i $b=0.1$, wykonaj:\n",
    "\n",
    "1. Oblicz gradienty (pochodne) w punktcie $w,b$ korzystając z implementacji `dw` i `db`.\n",
    "\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Wykonaj 100 takich  kroków z $\\alpha=0.1$. Na wykresie zobaczysz efekt takiego działania.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Implement the smallest fall algorithm. To do this, starting with $w=1$ and $b=0.1$, do:\n",
    "\n",
    "1. Calculate the gradients (derivatives) at the $w,b$ point using the implementation of `dw` and` db`.\n",
    "\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Follow 100 such steps with $\\alpha=0.1$. The graph will show the effect of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bcff12dc01b1005e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "w, b = 1, 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    pass\n",
    "### BEGIN SOLUTION\n",
    "    dw = 1/m*(w*x+b-y).dot(x)\n",
    "    db = 1/m*np.sum(w*x+b-y)\n",
    "\n",
    "    w = w - alpha*dw\n",
    "    b = b - alpha*db\n",
    "### END SOLUTION\n",
    "    \n",
    "print(L(w,b),w,b,dw,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ca4c968fd3208df7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal( 4.2030, w, significant=3)\n",
    "np.testing.assert_approx_equal( 1.0215, b, significant=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y,'b.')\n",
    "xlin = np.linspace(-2,2,55)\n",
    "plt.plot(xlin,w*xlin+b,'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl"
   },
   "source": [
    "### Dodatek - automatyczne obliczanie analitycznych gradientów w tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Appendix - automatic calculation of analytical gradients in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable([1.0],\"w\")\n",
    "b = tf.Variable(0.1,\"b\")\n",
    "x_tf = tf.placeholder(tf.float32, shape=(None,1))\n",
    "y_tf = tf.placeholder(tf.float32, shape=(None))\n",
    "\n",
    "\n",
    "lin_model =  tf.tensordot(x_tf,w,axes=[1,0]) + b\n",
    "\n",
    "loss = tf.reduce_mean( 0.5*(lin_model - y_tf)**2 )\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "dw, db = tf.gradients(ys=loss,xs=[w,b])\n",
    "\n",
    "sess.run([dw,db,loss],feed_dict={x_tf:x[:,np.newaxis],y_tf:y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(loss,feed_dict={x_tf:x[:,np.newaxis],y_tf:y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "pl"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
