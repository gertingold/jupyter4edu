{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6807741c0de771a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Regresja liniowa: *Stochastic Gradient Descent*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Linear regression: *Stochastic Gradient Descent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ac6d0b5a01ac4856",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import numpy as np \n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_orig = np.array([-2.29399323, -1.43363036, -0.52468804, -0.39544295, -0.24097318,\n",
    "       -0.14898657, -0.0343155 ,  0.19022609,  0.38726758,  0.59190507,\n",
    "        0.91906829,  1.03690893])\n",
    "\n",
    "y_orig = np.array([-7.77733551, -4.70740336, -2.41251556,  0.36564371, -1.9492586 ,\n",
    "       -0.19388007,  0.82003484,  2.6322221 ,  2.26459065,  2.97531505,\n",
    "        4.8613992 ,  7.20417432])\n",
    "# y = 4.2*x + 1.2 + np.random.randn(m)\n",
    "x, y = x_orig, y_orig\n",
    "\n",
    "m = x.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 1, 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1491c87bcdad133e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Funkcja straty\n",
    "\n",
    "Funkcją straty będzie suma kwadratów odchyleń przewidywania modelu od rzeczywistej wartości:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "The loss function will be the sum of squares of deviations of the model prediction from the actual value:\n",
    "\n",
    "$$ L = \\frac{1}{2m} \\sum_{i=0}^{m-1} (w x_i+b - y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d769269946a18db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "L = lambda w,b: 0.5/m*np.sum( (w*x+b-y)**2 )\n",
    "L(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pl",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8bd98ef0ec8b7a4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Zadanie 1\n",
    "\n",
    "Zaimplementuj stochastyczny algorytm najszybszego spadku. W tym celu startując z $w=1$ i $b=0.1$, wykonaj:\n",
    "\n",
    "1. Wybierz próbkę $n$ danych (batch). $n<=m$ \n",
    "2. Oblicz gradienty (pochodne) w nowym punktcie $w,b$ korzystając z implementacji `dw` i `db`.\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "\n",
    "Wykonaj 100 takich  kroków z $\\alpha=0.1$. Na wykresie zobaczysz efekt takiego działania.\n",
    "\n",
    "Wynik testu będzie poprawny jesli `n=10`\n",
    "\n",
    "\n",
    "Mamy:\n",
    "\n",
    " - $n=1$ Stochastic Gradient Descent (vanilla)\n",
    " - $1<n<m$ minibatch SGD\n",
    " - $n=m$ Gradient Descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Task 1\n",
    "\n",
    "Implement the  stochastic gradient descent. To do this, starting with $w=1$ and $b=0.1$, do:\n",
    "\n",
    "1. Select a sample $n$ data (batch). $n<=m$\n",
    "2. Calculate gradients (derivatives) at the new $w,b$ point using the implementation of `dw` and` db`.\n",
    "1. $w \\to w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "2. $b \\to b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "\n",
    "Follow 100 such steps with $\\alpha=0.1$. The graph will show the effect of this.\n",
    "\n",
    "The test result will be correct if `n = 10`\n",
    "\n",
    "\n",
    "we have:\n",
    "\n",
    " - $n=1$ Stochastic Gradient Descent (vanilla)\n",
    " - $1<n<m$ minibatch SGD\n",
    " - $n=m$ Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 1, 0.1\n",
    "x, y = x_orig, y_orig\n",
    "plt.figure()\n",
    "plt.plot(x,y,'b.')\n",
    "xlin = np.linspace(-2,2,55)\n",
    "l = plt.plot(xlin,w*xlin+b,'r-')[0]\n",
    "ax = plt.gca()\n",
    "fig = plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec12ff6d20fa3276",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "alpha = 0.1\n",
    "w, b = 1, 0.1\n",
    "hist = []\n",
    "for i in range(100):    \n",
    "\n",
    "    ith = [ 3,4  ] ## randomize!\n",
    "    x = None\n",
    "    y = None\n",
    "### BEGIN SOLUTION\n",
    "    ith = np.random.choice(m,n,replace=False)\n",
    "    x = x_orig[ith]\n",
    "    y = y_orig[ith]\n",
    "### END SOLUTION\n",
    "    \n",
    "    dw = 1/n*(w*x+b-y).dot(x)\n",
    "    db = 1/n*np.sum(w*x+b-y)\n",
    "\n",
    "    w = w - alpha*dw\n",
    "    b = b - alpha*db\n",
    "    \n",
    "    if i%10==0:\n",
    "        print(L(w,b),w,b,end='\\r')\n",
    "        l.set_data(xlin,w*xlin+b)\n",
    "        fig.canvas.draw()\n",
    "    hist.append(L(w,b))\n",
    "    #sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-29f6e61ba2c83670",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_approx_equal(w,4.2, significant=1)\n",
    "np.testing.assert_approx_equal(b,1.0, significant=1)\n",
    "assert x.shape[0] == n\n",
    "assert np.unique(x).shape[0] == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "pl",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "pl",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
